{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Advanced-Tensorflow-Specialization/blob/main/Caltech_Birds_Bounding_Box_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpiJj8ym0v0-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoilhmYe1b5t"
      },
      "outputs": [],
      "source": [
        "import os, re, time, json, zipfile\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khsTrAlcKXR5"
      },
      "source": [
        "## Download and Extract the Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-3-public/datasets/caltech_birds2010_011.zip"
      ],
      "metadata": {
        "id": "_5u7FnXWe75X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the data directory\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Create the data directory\n",
        "try:\n",
        "  os.mkdir(data_dir)\n",
        "except FileExistsError:\n",
        "  print(f'{data_dir} already exists')\n",
        "\n",
        "# Extract the dataset into the data directory\n",
        "with zipfile.ZipFile('./caltech_birds2010_011.zip') as zipref:\n",
        "  zipref.extractall(data_dir)"
      ],
      "metadata": {
        "id": "9CXzsZA8z86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmoFKEd98MP3"
      },
      "source": [
        "## Visualization Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhS3mNlDOLX"
      },
      "source": [
        "<a name=\"1-1\"></a>\n",
        "### 1.1 Bounding Boxes Utilities\n",
        "\n",
        "We have provided you with some functions which you will use to draw bounding boxes around the birds in the `image`.\n",
        "\n",
        "- `draw_bounding_box_on_image`: Draws a single bounding box on an image.\n",
        "- `draw_bounding_boxes_on_image`: Draws multiple bounding boxes on an image.\n",
        "- `draw_bounding_boxes_on_image_array`: Draws multiple bounding boxes on an array of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWIHFPa0uOC_"
      },
      "outputs": [],
      "source": [
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, color=(255, 0, 0), thickness=5):\n",
        "    \"\"\"\n",
        "    Adds a bounding box to an image.\n",
        "    Bounding box coordinates can be specified in either absolute (pixel) or\n",
        "    normalized coordinates by setting the use_normalized_coordinates argument.\n",
        "\n",
        "    Args:\n",
        "      image: a PIL.Image object.\n",
        "      ymin: ymin of bounding box.\n",
        "      xmin: xmin of bounding box.\n",
        "      ymax: ymax of bounding box.\n",
        "      xmax: xmax of bounding box.\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "    \"\"\"\n",
        "\n",
        "    image_width = image.shape[1]\n",
        "    image_height = image.shape[0]\n",
        "    cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, thickness)\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image(image, boxes, color=[], thickness=5):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on image.\n",
        "\n",
        "    Args:\n",
        "      image: a PIL.Image object.\n",
        "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "             The coordinates are in normalized format between [0, 1].\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if boxes is not a [N, 4] array\n",
        "    \"\"\"\n",
        "\n",
        "    boxes_shape = boxes.shape\n",
        "    if not boxes_shape:\n",
        "        return\n",
        "    if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
        "        raise ValueError('Input must be of size [N, 4]')\n",
        "    for i in range(boxes_shape[0]):\n",
        "        draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
        "                                 boxes[i, 2], color[i], thickness)\n",
        "\n",
        "\n",
        "def draw_bounding_boxes_on_image_array(image, boxes, color=[], thickness=5):\n",
        "    \"\"\"\n",
        "    Draws bounding boxes on image (numpy array).\n",
        "\n",
        "    Args:\n",
        "      image: a numpy array object.\n",
        "      boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
        "             The coordinates are in normalized format between [0, 1].\n",
        "      color: color to draw bounding box. Default is red.\n",
        "      thickness: line thickness. Default value is 4.\n",
        "      display_str_list_list: a list of strings for each bounding box.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if boxes is not a [N, 4] array\n",
        "    \"\"\"\n",
        "\n",
        "    draw_bounding_boxes_on_image(image, boxes, color, thickness)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USx9tRBF8hWy"
      },
      "source": [
        "<a name=\"1-2\"></a>\n",
        "### 1.2 Data and Predictions Utilities\n",
        "\n",
        "We've given you some helper functions and code that are used to visualize the data and the model's predictions.\n",
        "\n",
        "- `display_digits_with_boxes`: This displays a row of \"digit\" images along with the model's predictions for each image.\n",
        "- `plot_metrics`: This plots a given metric (like loss) as it changes over multiple epochs of training.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwJ4rZ1d_7ql"
      },
      "outputs": [],
      "source": [
        "# Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_digits_with_boxes(images, pred_bboxes, bboxes, iou, title, bboxes_normalized=False):\n",
        "\n",
        "    n = len(images)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 4))\n",
        "    plt.title(title)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "\n",
        "    for i in range(n):\n",
        "      ax = fig.add_subplot(1, 10, i+1)\n",
        "      bboxes_to_plot = []\n",
        "      if (len(pred_bboxes) > i):\n",
        "        bbox = pred_bboxes[i]\n",
        "        bbox = [bbox[0] * images[i].shape[1], bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0]]\n",
        "        bboxes_to_plot.append(bbox)\n",
        "\n",
        "      if (len(bboxes) > i):\n",
        "        bbox = bboxes[i]\n",
        "        if bboxes_normalized == True:\n",
        "          bbox = [bbox[0] * images[i].shape[1],bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0] ]\n",
        "        bboxes_to_plot.append(bbox)\n",
        "\n",
        "      img_to_draw = draw_bounding_boxes_on_image_array(image=images[i], boxes=np.asarray(bboxes_to_plot), color=[(255,0,0), (0, 255, 0)])\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "\n",
        "      plt.imshow(img_to_draw)\n",
        "\n",
        "      if len(iou) > i :\n",
        "        color = \"black\"\n",
        "        if (iou[i][0] < iou_threshold):\n",
        "          color = \"red\"\n",
        "        ax.text(0.2, -0.3, \"iou: %s\" %(iou[i][0]), color=color, transform=ax.transAxes)\n",
        "\n",
        "\n",
        "# utility to display training and validation curves\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,ylim)\n",
        "    plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkc7nzg-WUy"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "## 2. Preprocess and Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xv_8MbApX23"
      },
      "source": [
        "<a name=\"2-1\"></a>\n",
        "### 2.1 Preprocessing Utilities\n",
        "\n",
        "We have given you some helper functions to pre-process the image data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg66jHMvw_f8"
      },
      "source": [
        "#### read_image_tfds\n",
        "- Resizes `image` to (224, 224)\n",
        "- Normalizes `image`\n",
        "- Translates and normalizes bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEEyTpmNxS0A"
      },
      "outputs": [],
      "source": [
        "def read_image_tfds(image, bbox):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    shape = tf.shape(image)\n",
        "\n",
        "    factor_x = tf.cast(shape[1], tf.float32)\n",
        "    factor_y = tf.cast(shape[0], tf.float32)\n",
        "\n",
        "    image = tf.image.resize(image, (224, 224,))\n",
        "\n",
        "    image = image/127.5\n",
        "    image -= 1\n",
        "\n",
        "    bbox_list = [bbox[0] / factor_x ,\n",
        "                 bbox[1] / factor_y,\n",
        "                 bbox[2] / factor_x ,\n",
        "                 bbox[3] / factor_y]\n",
        "\n",
        "    return image, bbox_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxqvA3wkyH7p"
      },
      "source": [
        "#### read_image_with_shape\n",
        "This is very similar to `read_image_tfds` except it also keeps a copy of the original image (before pre-processing) and returns this as well.\n",
        "- Makes a copy of the original image.\n",
        "- Resizes `image` to (224, 224)\n",
        "- Normalizes `image`\n",
        "- Translates and normalizes bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f10wa31DyeQ4"
      },
      "outputs": [],
      "source": [
        "def read_image_with_shape(image, bbox):\n",
        "    original_image = image\n",
        "\n",
        "    image, bbox_list = read_image_tfds(image, bbox)\n",
        "\n",
        "    return original_image, image, bbox_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNEpxvyLykzo"
      },
      "source": [
        "#### read_image_tfds_with_original_bbox\n",
        "\n",
        "- This function reads `image` from `data`\n",
        "- It also denormalizes the bounding boxes (it undoes the bounding box normalization that is performed by the previous two helper functions.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsQo9vvhyoKb"
      },
      "outputs": [],
      "source": [
        "def read_image_tfds_with_original_bbox(data):\n",
        "    image = data[\"image\"]\n",
        "    bbox = data[\"bbox\"]\n",
        "\n",
        "    shape = tf.shape(image)\n",
        "    factor_x = tf.cast(shape[1], tf.float32)\n",
        "    factor_y = tf.cast(shape[0], tf.float32)\n",
        "\n",
        "    bbox_list = [bbox[1] * factor_x ,\n",
        "                 bbox[0] * factor_y,\n",
        "                 bbox[3] * factor_x,\n",
        "                 bbox[2] * factor_y]\n",
        "    return image, bbox_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElJ9VX0yui9"
      },
      "source": [
        "#### dataset_to_numpy_util\n",
        "This function converts a `dataset` into numpy arrays of images and boxes.\n",
        "- This will be used when visualizing the images and their bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF-luxkJyzIA"
      },
      "outputs": [],
      "source": [
        "def dataset_to_numpy_util(dataset, batch_size=0, N=0):\n",
        "\n",
        "    # eager execution: loop through datasets normally\n",
        "    take_dataset = dataset.shuffle(1024)\n",
        "\n",
        "    if batch_size > 0:\n",
        "        take_dataset = take_dataset.batch(batch_size)\n",
        "\n",
        "    #.take - limit the dataset to the first N elements after the shuffling (and batching, if applicable).\n",
        "    if N > 0:\n",
        "        take_dataset = take_dataset.take(N)\n",
        "\n",
        "    if tf.executing_eagerly():\n",
        "        ds_images, ds_bboxes = [], []\n",
        "        for images, bboxes in take_dataset:\n",
        "            ds_images.append(images.numpy())\n",
        "            ds_bboxes.append(bboxes.numpy())\n",
        "\n",
        "    return (np.array(ds_images, dtype='object'), np.array(ds_bboxes, dtype='object'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZSf8zvBy2RX"
      },
      "source": [
        "#### dataset_to_numpy_with_original_bboxes_util\n",
        "\n",
        "- This function converts a `dataset` into numpy arrays of\n",
        "  - original images\n",
        "  - resized and normalized images\n",
        "  - bounding boxes\n",
        "- This will be used for plotting the original images with true and predicted bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE8dgyPC1_6m"
      },
      "outputs": [],
      "source": [
        "def dataset_to_numpy_with_original_bboxes_util(dataset, batch_size=0, N=0):\n",
        "\n",
        "    normalized_dataset = dataset.map(read_image_with_shape)\n",
        "    if batch_size > 0:\n",
        "        normalized_dataset = normalized_dataset.batch(batch_size)\n",
        "\n",
        "    if N > 0:\n",
        "        normalized_dataset = normalized_dataset.take(N)\n",
        "\n",
        "    if tf.executing_eagerly():\n",
        "        ds_original_images, ds_images, ds_bboxes = [], [], []\n",
        "\n",
        "    for original_images, images, bboxes in normalized_dataset:\n",
        "        ds_images.append(images.numpy())\n",
        "        ds_bboxes.append(bboxes.numpy())\n",
        "        ds_original_images.append(original_images.numpy())\n",
        "\n",
        "    return np.array(ds_original_images, dtype='object'), np.array(ds_images, dtype='object'), np.array(ds_bboxes, dtype='object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4gB0hprzMw4"
      },
      "source": [
        "<a name=\"2-2\"></a>\n",
        "### 2.2 Visualize the images and their bounding box labels\n",
        "Now you'll take a random sample of images from the training and validation sets and visualize them by plotting the corresponding bounding boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUPENeUHKXR6"
      },
      "source": [
        "Visualize the **training** images and their bounding box labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW_AyCNIKXR6"
      },
      "outputs": [],
      "source": [
        "def get_visualization_training_dataset():\n",
        "    dataset, info = tfds.load(\"caltech_birds2010\", split=\"train\", with_info=True, data_dir=data_dir, download=False)\n",
        "    print(info)\n",
        "    visualization_training_dataset = dataset.map(read_image_tfds_with_original_bbox,\n",
        "                                                 num_parallel_calls=16)\n",
        "    return visualization_training_dataset\n",
        "\n",
        "\n",
        "visualization_training_dataset = get_visualization_training_dataset()\n",
        "\n",
        "\n",
        "(visualization_training_images, visualization_training_bboxes) = dataset_to_numpy_util(visualization_training_dataset, N=10)\n",
        "display_digits_with_boxes(np.array(visualization_training_images), np.array([]), np.array(visualization_training_bboxes), np.array([]), \"training images and their bboxes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qCuoUtYKXR6"
      },
      "source": [
        "Visualize the **validation** images and their bounding boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLGiEyK_KXR6"
      },
      "outputs": [],
      "source": [
        "def get_visualization_validation_dataset():\n",
        "    dataset = tfds.load(\"caltech_birds2010\", split=\"test\", data_dir=data_dir, download=False)\n",
        "    visualization_validation_dataset = dataset.map(read_image_tfds_with_original_bbox, num_parallel_calls=16)\n",
        "    return visualization_validation_dataset\n",
        "\n",
        "\n",
        "visualization_validation_dataset = get_visualization_validation_dataset()\n",
        "\n",
        "(visualization_validation_images, visualization_validation_bboxes) = dataset_to_numpy_util(visualization_validation_dataset, N=10)\n",
        "display_digits_with_boxes(np.array(visualization_validation_images), np.array([]), np.array(visualization_validation_bboxes), np.array([]), \"validation images and their bboxes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2f2DWcnzZRq"
      },
      "source": [
        "<a name=\"2-3\"></a>\n",
        "### 2.3 Load and prepare the datasets for the model\n",
        "\n",
        "These next two functions read and prepare the datasets that you'll feed to the model.\n",
        "- They use `read_image_tfds` to resize, and normalize each image and its bounding box label.\n",
        "- They performs shuffling and batching.\n",
        "- You'll use these functions to create `training_dataset` and `validation_dataset`, which you will give to the model that you're about to build."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5shayI_tzdq0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "def get_training_dataset(dataset):\n",
        "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "    dataset = dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(-1)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(dataset):\n",
        "    dataset = dataset.map(read_image_tfds, num_parallel_calls=16)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.repeat()\n",
        "    return dataset\n",
        "\n",
        "training_dataset = get_training_dataset(visualization_training_dataset)\n",
        "validation_dataset = get_validation_dataset(visualization_validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8nHWWkS_eeZ"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "## 3. Define the Network\n",
        "\n",
        "Bounding box prediction is treated as a \"regression\" task, in that you want the model to output numerical values.\n",
        "\n",
        "- You will be performing transfer learning with **MobileNet V2**.  The model architecture is available in TensorFlow Keras.\n",
        "- You'll also use pretrained `'imagenet'` weights as a starting point for further training.  These weights are also readily available\n",
        "- You will choose to retrain all layers of **MobileNet V2** along with the final classification layers.\n",
        "\n",
        "**Note:** For the following exercises, please use the TensorFlow Keras Functional API (as opposed to the Sequential API)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DFecRhe0Pqc"
      },
      "outputs": [],
      "source": [
        "def feature_extractor(inputs):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # Create a mobilenet version 2 model object\n",
        "    mobilenet_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
        "        input_shape=(224, 224, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # pass the inputs into this model object to get a feature extractor for these inputs\n",
        "    feature_extractor = mobilenet_model(inputs)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # return the feature_extractor\n",
        "    return feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0njchQxB0b4Q"
      },
      "outputs": [],
      "source": [
        "def dense_layers(features):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # global average pooling 2d layer\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
        "\n",
        "    # flatten layer\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "    # 1024 Dense layer, with relu\n",
        "    x = tf.keras.layers.Dense(1024,activation = 'relu')(x)\n",
        "\n",
        "    # 512 Dense layer, with relu\n",
        "    x = tf.keras.layers.Dense(512,activation = 'relu')(x)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdsD0-Jl07zW"
      },
      "outputs": [],
      "source": [
        "def bounding_box_regression(x):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # Dense layer named `bounding_box`\n",
        "    bounding_box_regression_output = tf.keras.layers.Dense(4,name = 'bounding_box')(x)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return bounding_box_regression_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn9O9c7I1XRJ"
      },
      "outputs": [],
      "source": [
        "def final_model(inputs):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # features\n",
        "    feature_cnn = feature_extractor(inputs)\n",
        "\n",
        "    # dense layers\n",
        "    last_dense_layer = dense_layers(feature_cnn)\n",
        "\n",
        "    # bounding box\n",
        "    bounding_box_output = bounding_box_regression(last_dense_layer)\n",
        "\n",
        "    # define the TensorFlow Keras model using the inputs and outputs to your model\n",
        "    model = tf.keras.Model(inputs = inputs,outputs = bounding_box_output)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C67ZmsTe1n9m"
      },
      "outputs": [],
      "source": [
        "def define_and_compile_model():\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # define the input layer\n",
        "    inputs = tf.keras.Input(shape = (224,224,3))\n",
        "\n",
        "    # create the model\n",
        "    model = final_model(inputs)\n",
        "\n",
        "    # compile your model\n",
        "    model.compile(\n",
        "        optimizer = tf.keras.optimizers.SGD(momentum = 0.9),\n",
        "        loss = 'mse'\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56y8UNFQIVwj"
      },
      "outputs": [],
      "source": [
        "# define your model\n",
        "model = define_and_compile_model()\n",
        "# print model layers\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtVVYVlvKXR7"
      },
      "source": [
        "<a name='4'></a>\n",
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoIY6xQ_KXR7"
      },
      "outputs": [],
      "source": [
        "# You'll train 50 epochs\n",
        "EPOCHS = 50\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "# Choose a batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Get the length of the training set\n",
        "length_of_training_dataset = len(visualization_training_dataset)\n",
        "\n",
        "# Get the length of the validation set\n",
        "length_of_validation_dataset = len(visualization_validation_dataset)\n",
        "\n",
        "# Get the steps per epoch (may be a few lines of code)\n",
        "steps_per_epoch = length_of_training_dataset // BATCH_SIZE\n",
        "\n",
        "# get the validation steps (per epoch) (may be a few lines of code)\n",
        "validation_steps = length_of_validation_dataset//BATCH_SIZE\n",
        "if length_of_validation_dataset % BATCH_SIZE > 0:\n",
        "    validation_steps += 1\n",
        "\n",
        "### END CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCj8CYtSQpY"
      },
      "source": [
        "### 4.2 Fit the model to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTwH_P-ZJ_xx"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ####\n",
        "\n",
        "# Fit the model, setting the parameters noted in the instructions above.\n",
        "history = model.fit(\n",
        "    training_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aBzmycIsO8w"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5. Validate the Model\n",
        "\n",
        "<a name='5-1'></a>\n",
        "### 5.1 Loss\n",
        "\n",
        "You can now evaluate your trained model's performance by checking its loss value on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWbkUql5sAok"
      },
      "outputs": [],
      "source": [
        "loss = model.evaluate(validation_dataset, steps=validation_steps)\n",
        "print(\"Loss: \", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7E81sgUsUC4"
      },
      "source": [
        "<a name='5-2'></a>\n",
        "### 5.2 Plot Loss Function\n",
        "\n",
        "You can also plot the loss metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz-b8TxU6EDj"
      },
      "outputs": [],
      "source": [
        "plot_metrics(\"loss\", \"Bounding Box Loss\", ylim=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G7KFVX9sXJt"
      },
      "source": [
        "<a name='5-3'></a>\n",
        "### 5.3 Evaluate performance using IoU\n",
        "\n",
        "You can see how well your model predicts bounding boxes on the validation set by calculating the Intersection-over-union (IoU) score for each image.\n",
        "\n",
        "- You'll find the IoU calculation implemented for you.\n",
        "- Predict on the validation set of images.\n",
        "- Apply the `intersection_over_union` on these predicted bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFqJxt3_VrCm"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(pred_box, true_box):\n",
        "\n",
        "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
        "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
        "\n",
        "    #Calculate coordinates of overlap area between boxes\n",
        "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
        "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
        "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
        "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
        "\n",
        "    #Calculates area of true and predicted boxes\n",
        "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
        "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
        "\n",
        "    #Calculates overlap area and union area.\n",
        "    overlap_area = np.maximum((xmax_overlap - xmin_overlap),0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
        "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
        "\n",
        "    # Defines a smoothing factor to prevent division by 0\n",
        "    smoothing_factor = 1e-10\n",
        "\n",
        "    #Updates iou score\n",
        "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
        "\n",
        "    return iou\n",
        "\n",
        "#Makes predictions\n",
        "original_images, normalized_images, normalized_bboxes = dataset_to_numpy_with_original_bboxes_util(visualization_validation_dataset, N=500)\n",
        "predicted_bboxes = model.predict(normalized_images.astype('float32'))\n",
        "\n",
        "\n",
        "#Calculates IOU and reports true positives and false positives based on IOU threshold\n",
        "iou = intersection_over_union(predicted_bboxes, normalized_bboxes)\n",
        "iou_threshold = 0.5\n",
        "\n",
        "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
        "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFVovcUUVs1"
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6. Visualize Predictions\n",
        "\n",
        "Lastly, you'll plot the predicted and ground truth bounding boxes for a random set of images and visually see how well you did!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR9Bb4uCwTyw"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "indexes = np.random.choice(len(predicted_bboxes), size=n)\n",
        "\n",
        "iou_to_draw = iou[indexes]\n",
        "norm_to_draw = original_images[indexes]\n",
        "display_digits_with_boxes(original_images[indexes], predicted_bboxes[indexes], normalized_bboxes[indexes], iou[indexes], \"True and Predicted values\", bboxes_normalized=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}