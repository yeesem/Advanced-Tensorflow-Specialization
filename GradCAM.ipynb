{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2bae/TjBWFXPU1PJ0rO2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Advanced-Tensorflow-Specialization/blob/main/GradCAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "DYbl3HX2qbj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXwcYkQvqXNa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from skimage.io import imread,imsave\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD,Adam,RMSprop\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Prepare the Dataset"
      ],
      "metadata": {
        "id": "kzfl_8WqtAAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.disable_progress_bar()\n",
        "\n",
        "splits = ['train[:80%]','train[80%:90%]','train[90%:]']\n",
        "\n",
        "# Load the dataset given the splits defined above\n",
        "splits,info = tfds.load('cats_vs_dogs',with_info = True,as_supervised = True,split = splits)\n",
        "\n",
        "(train_examples,validation_examples,test_examples) = splits\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Auyh9VyjtFXW",
        "outputId": "62a2898a-1368-40a1-c049-b7fa5e68d818"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:1738 images were corrupted and were skipped\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (224,224)\n",
        "\n",
        "# resizes the image and normalized the pixel values\n",
        "def format_image(image,label):\n",
        "  imag = tf.image.resize(image,IMAGE_SIZE) / 255.0\n",
        "  return imag,label\n",
        "\n",
        "# Prepare batches\n",
        "# prefetch - is used as part of the data pipeline to improve the efficiency of data loading.\n",
        "# autotune - tells TensorFlow to automatically tune the prefetch buffer size\n",
        "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_batches = test_examples.map(format_image).batch(1)"
      ],
      "metadata": {
        "id": "zMcBN3NNwUiU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "TWV6xs3Tx3bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  # Load the base VGG model\n",
        "  base_model = vgg16.VGG16(\n",
        "      input_shape = IMAGE_SIZE + (3,),\n",
        "      weights = 'imagenet',\n",
        "      include_top = False\n",
        "  )\n",
        "\n",
        "  # Add a GAP layer\n",
        "  output = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "\n",
        "  # Output has two neurons for the 2 classes\n",
        "  output = layers.Dense(2,activation = 'softmax')(output)\n",
        "\n",
        "  # Set the inputs and outputs of the model\n",
        "  model = Model(base_model.input,output)\n",
        "\n",
        "  # Freeze the earlier layers\n",
        "  for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # choose the optimizer\n",
        "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "  # Configure the model for training\n",
        "  model.compile(\n",
        "      loss = 'sparse_categorical_crossentropy',\n",
        "      optimizer = optimizer,\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "WHz_kKbhx2-U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk62g3pL2C_i",
        "outputId": "138ceba1-03f7-4bd1-c2a6-2cd5df32f3b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 512)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14715714 (56.14 MB)\n",
            "Trainable params: 7080450 (27.01 MB)\n",
            "Non-trainable params: 7635264 (29.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "model.fit(\n",
        "    train_batches,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = validation_batches\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Kmri6T2E_g",
        "outputId": "40d53d5c-da4d-41ad-be3c-3aa17f4c63ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "582/582 [==============================] - 132s 199ms/step - loss: 0.9961 - accuracy: 0.7418 - val_loss: 0.1394 - val_accuracy: 0.9398\n",
            "Epoch 2/3\n",
            "582/582 [==============================] - 107s 170ms/step - loss: 0.1456 - accuracy: 0.9434 - val_loss: 0.1388 - val_accuracy: 0.9445\n",
            "Epoch 3/3\n",
            "582/582 [==============================] - 108s 170ms/step - loss: 0.0989 - accuracy: 0.9628 - val_loss: 0.1172 - val_accuracy: 0.9592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78a90fedf790>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Interpretability"
      ],
      "metadata": {
        "id": "_AAn6Q3S2gfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all the layers for which you want to visualize the outputs and store it in a list\n",
        "outputs = [layer.output for layer in model.layers[1:18]]\n",
        "\n",
        "# Define a new model that generates the above output\n",
        "vis_model = Model(model.input,outputs)\n",
        "\n",
        "# Store the layer names we are interested in\n",
        "layer_names = []\n",
        "for layer in outputs:\n",
        "  layer_names.append(layer.name.split(\"/\")[0])\n",
        "\n",
        "print(\"Layers that will be used for visualization : \")\n",
        "print(layer_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzKKehbY2i-F",
        "outputId": "e4794158-b573-4f0a-ddc0-ac2e9bd15d77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layers that will be used for visualization : \n",
            "['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class activation maps (GradCAM)"
      ],
      "metadata": {
        "id": "BV2cfpAg3xQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_CAM(preprocessed_image,actual_label,layer_name = 'block5_conv3'):\n",
        "  model_grad = Model(\n",
        "      [model.inputs],\n",
        "      [model.get_layer(layer_name).output,model.output]\n",
        "  )\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    conv_output_values,predictions = model_grad(preprocessed_image)\n",
        "\n",
        "    # watch the conv_output_values\n",
        "    tape.watch(conv_output_values)\n",
        "\n",
        "    # Use binary cross entropy loss\n",
        "    # actual label is 0 if cat, 1 is dog\n",
        "    # get prediction probability of dog\n",
        "    # If model does well\n",
        "    # pred_prob should be close to 0 if cat,close to 1 if dog\n",
        "    pred_prob = predictions[:,1]\n",
        "\n",
        "    # Make sure actual label is a float, like the rest of the loss calculation\n",
        "    actual_label = tf.cast(actual_label,dtype = tf.float32)\n",
        "\n",
        "    # Add a tiny value to avoid log of 0\n",
        "    smoothing = 0.00001\n",
        "\n",
        "    # calculate the loss as binary cross entropy\n",
        "    loss = -1 * (actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing))\n",
        "    print(\"Binary loss : \",{loss})\n",
        "\n",
        "  # get the gradient of the loss with respect to the outputs of the last conv layer\n",
        "  grad_values = tape.gradient(loss,conv_output_values)\n",
        "  grad_values = K.mean(grad_values,axis = (0,1,2))\n",
        "\n",
        "  conv_output_values = np.squeeze(conv_output_values.numpy())\n",
        "  grads_values = grad_values.numpy()\n",
        "\n",
        "  # weight the convolutional outputs with the computed gradients\n",
        "  for i in range(512):\n",
        "    conv_output_values[:,:,i] += grads_values[i]\n",
        "\n",
        "  # Exp - 3D tensor with shape (14, 14, 512)\n",
        "  # After mean(axis = -1) - shape (14, 14)\n",
        "  heatmap = np.mean(conv_output_values,axis = -1)\n",
        "  # It is a relu activation function\n",
        "  # Negative values in heatmap can be less meaningful for visualizing areas of interest in the image\n",
        "  heatmap = np.maximum(heatmap,0)\n",
        "  # Normalize the heatmap to range [0,1]\n",
        "  heatmap /= heatmap.max()\n",
        "\n",
        "  del model_grad,conv_output_values,grads_values,loss\n",
        "\n",
        "  return heatmap"
      ],
      "metadata": {
        "id": "CLIcePXG31nu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample(idx = None):\n",
        "  if idx:\n",
        "    for img,label in test_batches.take(idx):\n",
        "      sample_image = img[0]\n",
        "      sample_label = label[0]\n",
        "\n",
        "  else:\n",
        "    for img,label in test_batches.shuffle(1000).take(1):\n",
        "      sample_image = img[0]\n",
        "      sample_label = label[0]\n",
        "\n",
        "  sample_image_processed = np.expand_dims(sample_image,axis = 0)\n",
        "\n",
        "  activations = vis_model.predict(sample_image_precessed)\n",
        "\n",
        "  pred_label = np.argmax(model.predict(sample_image_processed),axis = -1)[0]\n",
        "\n",
        "  sample_activation -= sample_activation.mean()\n",
        "  sample_activation /= sample_activation.std()\n",
        "\n",
        "  sample_activation *= 255\n",
        "  sample_activation = np.clip(\n",
        "      sample_activation,0,255\n",
        "  ).astype(np.uint8)\n",
        "\n",
        "  heatmap = get_CAM(\n",
        "      sample_image_processed,\n",
        "      sample_label\n",
        "  )\n",
        "\n",
        "  heatmap = cv2.resize(\n",
        "      heatmap,\n",
        "      (sample_image.shape[0],sample_image.shape[1])\n",
        "  )\n",
        "  heatmap = heatmap * 255\n",
        "  heatmap = np.clip(heatmap,0,255).astype(np.uint8)\n",
        "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_HOT)\n",
        "  converted_img = sample_image.numpy()\n",
        "  super_imposed_image = cv2.addWeighted(converted_img, 0.8, heatmap.astype('float32'), 2e-3, 0.0)\n",
        "\n",
        "  f,ax = plt.subplots(2,2,figsize = (15,8))\n",
        "\n",
        "  ax[0,0].imshow(sample_image)\n",
        "  ax[0,0].set_title(f\"True label : {sample_label} \\n Predicted label : {pred_label}\")\n",
        "  ax[0,0].axis(\"off\")\n",
        "\n",
        "  ax[0,1].imshow(sample_activation)\n",
        "  ax[0,1].set_title(\"Random feature map\")\n",
        "  ax[0,1].axis(\"off\")\n",
        "\n",
        "  ax[1,0].imshow(heatmap)\n",
        "  ax[1,0].set_title(\"Class Activation Map\")\n",
        "  ax[1,0].axis(\"off\")\n",
        "\n",
        "  ax[1,1].imshow(super_imposed_image)\n",
        "  ax[1,1].set_title(\"Activation map superimposed\")\n",
        "  ax[1,1].axis(\"off\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return activations"
      ],
      "metadata": {
        "id": "z0vouGgF8DSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Visualization"
      ],
      "metadata": {
        "id": "KvNxXm5CIJk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose an image index to show, or leave it as None to get a random image\n",
        "activations = show_sample(idx = None)"
      ],
      "metadata": {
        "id": "XqPGCFNyIMGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate activations of layers"
      ],
      "metadata": {
        "id": "xqeKnuXvIbxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_intermediate_activations(layer_names,activations):\n",
        "  assert len(layer_names) == len(activations), \"Make sure layers and activation value match\"\n",
        "  images_per_row = 16\n",
        "\n",
        "  for layer_name,layer_activation in zip(layer_names,activations):\n",
        "    nb_features = layer_activation.shape[-1]\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    nb_cols = nb_features // images_per_row\n",
        "    grid = np.zeros((size*nb_cols,size*images_per_row))\n",
        "\n",
        "    for col in range(nb_cols):\n",
        "      for row in range(images_per_row):\n",
        "        feature_map = layer_activation[0,:,:,col*images_per_row + row]\n",
        "        feature_map -= feature_map.mean()\n",
        "        feature_map /= feature_map.std()\n",
        "        feature_map *= 255\n",
        "        feature_map = np.clip(faeture_map,0.255).astype(np.uint8)\n",
        "\n",
        "        grid[col*size:(col+1)*size,row*size:(row+1)*size] = feature_map\n",
        "\n",
        "      scale = 1./size\n",
        "      plt.figure(figsize = (scale8grid.shape[1],scale*grid.shape[0]))\n",
        "      plt.title(layer_name)\n",
        "      plt.grid(False)\n",
        "      plt.axis('off')\n",
        "      plt.imshow(grid,aspect = 'auto',cmap = 'viridis')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V3P1kJRdIezs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_intermediate_activations(activations = activations,\n",
        "                                   layer_names = layer_names)"
      ],
      "metadata": {
        "id": "riOxve_yK14j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}