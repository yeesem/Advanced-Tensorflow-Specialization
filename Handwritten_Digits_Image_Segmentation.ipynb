{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Advanced-Tensorflow-Specialization/blob/main/Handwritten_Digits_Image_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ3g9dJxSxmN"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aifz2907kxYN"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RYh6cCzXE6R"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROok0i9rMcu0"
      },
      "outputs": [],
      "source": [
        "# download zipped dataset\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/tensorflow-1-public/tensorflow-3-temp/m2nist.zip \\\n",
        "    -O /tmp/m2nist.zip\n",
        "\n",
        "# find and extract to a local folder ('/tmp/training')\n",
        "local_zip = '/tmp/m2nist.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/training')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy17LYR7XJNa"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy_pw5I2-xLP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def read_image_and_annotation(image, annotation):\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.reshape(image, (image.shape[0], image.shape[1], 1,))\n",
        "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
        "  # Dividing by 127.5 and subtracting by 1 scales the pixel values to the range [-1, 1].\n",
        "  # This is a common normalization technique in neural networks to improve training stability and performance.\n",
        "  # Divide by 127.5:\n",
        "  # If you divide 0 by 127.5, you get 0.\n",
        "  #  If you divide 127.5 by 127.5, you get 1.\n",
        "  #  If you divide 255 by 127.5, you get 2.\n",
        "  #  Subtract by 1:\n",
        "  #  Subtracting 1 from the above results:\n",
        "  #  0 becomes -1.\n",
        "  #  1 becomes 0.\n",
        "  #  2 becomes 1.\n",
        "  #    image = image / 127.5\n",
        "  #    image -= 1\n",
        "  return image, annotation\n",
        "\n",
        "def get_training_dataset(images, annos):\n",
        "\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  training_dataset = training_dataset.map(read_image_and_annotation)\n",
        "\n",
        "  training_dataset = training_dataset.shuffle(512, reshuffle_each_iteration=True)\n",
        "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
        "  training_dataset = training_dataset.repeat()\n",
        "  # use -1 directly, it achieves the same effect as tf.data.experimental.AUTOTUNE.\n",
        "  training_dataset = training_dataset.prefetch(-1)\n",
        "\n",
        "  return training_dataset\n",
        "\n",
        "\n",
        "def get_validation_dataset(images, annos):\n",
        "  validation_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  validation_dataset = validation_dataset.map(read_image_and_annotation)\n",
        "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "  validation_dataset = validation_dataset.repeat()\n",
        "\n",
        "  return validation_dataset\n",
        "\n",
        "\n",
        "def get_test_dataset(images, annos):\n",
        "\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((images, annos))\n",
        "  test_dataset = test_dataset.map(read_image_and_annotation)\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "\n",
        "def load_images_and_segments():\n",
        "\n",
        "  #Loads images and segmentation masks.\n",
        "  images = np.load('/tmp/training/combined.npy')\n",
        "  segments = np.load('/tmp/training/segmented.npy')\n",
        "\n",
        "  # Makes training, validation, test splits from loaded images and segmentation masks.\n",
        "  # test_size=0.2 means that 20% of the data will be used for testing and 80% for training.\n",
        "  train_images, val_images, train_annos, val_annos = train_test_split(images, segments, test_size=0.2, shuffle=True)\n",
        "  val_images, test_images, val_annos, test_annos = train_test_split(val_images, val_annos, test_size=0.2, shuffle=True)\n",
        "\n",
        "  return (train_images, train_annos), (val_images, val_annos), (test_images, test_annos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIS70_um_Y7n",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "train_slices, val_slices, test_slices = load_images_and_segments()\n",
        "\n",
        "# Create training, validation, test datasets.\n",
        "training_dataset = get_training_dataset(train_slices[0], train_slices[1])\n",
        "validation_dataset = get_validation_dataset(val_slices[0], val_slices[1])\n",
        "test_dataset = get_test_dataset(test_slices[0], test_slices[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKXJYZi7A0dF"
      },
      "source": [
        "## Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "d46YCbvPafbp"
      },
      "outputs": [],
      "source": [
        "# Visualization Utilities\n",
        "\n",
        "# there are 11 classes in the dataset: one class for each digit (0 to 9) plus the background class\n",
        "n_classes = 11\n",
        "\n",
        "# assign a random color for each class\n",
        "colors = [tuple(np.random.randint(256, size=3) / 255.0) for i in range(n_classes)]\n",
        "\n",
        "def fuse_with_pil(images):\n",
        "\n",
        "  widths = (image.shape[1] for image in images)\n",
        "  heights = (image.shape[0] for image in images)\n",
        "  total_width = sum(widths)\n",
        "  max_height = max(heights)\n",
        "\n",
        "  new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "  x_offset = 0\n",
        "  for im in images:\n",
        "    pil_image = PIL.Image.fromarray(np.uint8(im))\n",
        "    new_im.paste(pil_image, (x_offset,0))\n",
        "    x_offset += im.shape[1]\n",
        "\n",
        "  return new_im\n",
        "\n",
        "\n",
        "def give_color_to_annotation(annotation):\n",
        "\n",
        "  seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
        "\n",
        "  for c in range(n_classes):\n",
        "    segc = (annotation == c)\n",
        "    seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
        "    seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
        "    seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
        "\n",
        "  return seg_img\n",
        "\n",
        "\n",
        "def show_annotation_and_prediction(image, annotation, prediction, iou_list, dice_score_list):\n",
        "\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  true_img = give_color_to_annotation(new_ann)\n",
        "  pred_img = give_color_to_annotation(prediction)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "  image = np.uint8(image)\n",
        "  images = [image, np.uint8(pred_img), np.uint8(true_img)]\n",
        "\n",
        "  metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0 and idx < 10]\n",
        "  metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
        "\n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\".join(display_string_list)\n",
        "\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  for idx, im in enumerate(images):\n",
        "    plt.subplot(1, 3, idx+1)\n",
        "    if idx == 1:\n",
        "      plt.xlabel(display_string)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(im)\n",
        "\n",
        "\n",
        "def show_annotation_and_image(image, annotation):\n",
        "\n",
        "  new_ann = np.argmax(annotation, axis=2)\n",
        "  seg_img = give_color_to_annotation(new_ann)\n",
        "\n",
        "  image = image + 1\n",
        "  image = image * 127.5\n",
        "  image = np.reshape(image, (image.shape[0], image.shape[1],))\n",
        "\n",
        "  image = np.uint8(image)\n",
        "  images = [image, seg_img]\n",
        "\n",
        "  images = [image, seg_img]\n",
        "  fused_img = fuse_with_pil(images)\n",
        "  plt.imshow(fused_img)\n",
        "\n",
        "\n",
        "def list_show_annotation(dataset, num_images):\n",
        "\n",
        "  ds = dataset.unbatch()\n",
        "\n",
        "  plt.figure(figsize=(20, 15))\n",
        "  plt.title(\"Images And Annotations\")\n",
        "  plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
        "\n",
        "  for idx, (image, annotation) in enumerate(ds.take(num_images)):\n",
        "    plt.subplot(5, 5, idx + 1)\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    show_annotation_and_image(image.numpy(), annotation.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFO_hIhLWYT4"
      },
      "outputs": [],
      "source": [
        "# get 10 images from the training set\n",
        "list_show_annotation(training_dataset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdgVkp8wZua0"
      },
      "outputs": [],
      "source": [
        "# get 10 images from the validation set\n",
        "list_show_annotation(validation_dataset, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv2k8xabRb8"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHlBUZvsDybt"
      },
      "source": [
        "### Define the Basic Convolution Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azEEVytHR0Kn"
      },
      "outputs": [],
      "source": [
        "# parameter describing where the channel dimension is found in our dataset\n",
        "IMAGE_ORDERING = 'channels_last'\n",
        "\n",
        "def conv_block(input, filters, kernel_size, pooling_size, pool_strides):\n",
        "  # use the functional syntax to stack the layers as shown in the diagram above\n",
        "  x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', data_format=IMAGE_ORDERING)(input)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', data_format = IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.LeakyReLU()(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pooling_size,pool_strides,data_format = IMAGE_ORDERING)(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-jJbC91EXTV"
      },
      "source": [
        "### Define the Downsampling Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2VNB99LRwQr"
      },
      "outputs": [],
      "source": [
        "def FCN8(input_height=64, input_width=84):\n",
        "\n",
        "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 1))\n",
        "\n",
        "    # pad the input image width to 96 pixels\n",
        "    x = tf.keras.layers.ZeroPadding2D(((0, 0), (0, 96-input_width)))(img_input)\n",
        "\n",
        "    # Block 1\n",
        "    x = conv_block(x, filters = 32, kernel_size = 3, pooling_size = 2,pool_strides = 2)\n",
        "\n",
        "    # Block 2\n",
        "    x = conv_block(x, filters = 64, kernel_size = 3, pooling_size = 2, pool_strides = 2)\n",
        "\n",
        "    # Block 3\n",
        "    x = conv_block(x, filters = 128, kernel_size = 3, pooling_size = 2, pool_strides = 2)\n",
        "    # save the feature map at this stage\n",
        "    f3 = x\n",
        "\n",
        "    # Block 4\n",
        "    x = conv_block(x, filters = 256, kernel_size = 3, pooling_size = 2, pool_strides = 2)\n",
        "    # save the feature map at this stage\n",
        "    f4 = x\n",
        "\n",
        "    # Block 5\n",
        "    x = conv_block(x, filters = 256, kernel_size = 3, pooling_size = 2,pool_strides = 2)\n",
        "    # save the feature map at this stage\n",
        "    f5 = x\n",
        "\n",
        "    return (f3, f4, f5), img_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbjYEQU8Eq-T"
      },
      "source": [
        "### Define the FCN-8 decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giYEct_Se5Xj"
      },
      "outputs": [],
      "source": [
        "def fcn8_decoder(convs, n_classes):\n",
        "  # features from the encoder stage\n",
        "  f3, f4, f5 = convs\n",
        "\n",
        "  # number of filters\n",
        "  n = 512\n",
        "\n",
        "  # add convolutional layers on top of the CNN extractor.\n",
        "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
        "  o = tf.keras.layers.Dropout(0.5)(o)\n",
        "\n",
        "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
        "\n",
        "  # Upsample `o` above and crop any extra pixels introduced\n",
        "  o = tf.keras.layers.Conv2DTranspose(\n",
        "      n_classes,\n",
        "      kernel_size = (4,4),\n",
        "      strides = (2,2),\n",
        "      use_bias = False,\n",
        "      data_format = IMAGE_ORDERING\n",
        "  )(o)\n",
        "  o = tf.keras.layers.Cropping2D(cropping = (1,1))(o)\n",
        "\n",
        "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
        "  o2 = f4\n",
        "  o2 = tf.keras.layers.Conv2D(\n",
        "      n_classes,\n",
        "      kernel_size = (1,1),\n",
        "      activation = 'relu',\n",
        "      padding = 'same',\n",
        "      data_format = IMAGE_ORDERING\n",
        "  )(o2)\n",
        "\n",
        "  # add the results of the upsampling and pool 4 prediction\n",
        "  o = tf.keras.layers.Add()([o2,o])\n",
        "\n",
        "  # upsample the resulting tensor of the operation you just did\n",
        "  o = tf.keras.layers.Conv2DTranspose(\n",
        "      n_classes,\n",
        "      kernel_size = (4,4),\n",
        "      strides = (2,2),\n",
        "      use_bias = False,\n",
        "      data_format = IMAGE_ORDERING\n",
        "  )(o)\n",
        "  o = tf.keras.layers.Cropping2D(cropping = (1,1))(o)\n",
        "\n",
        "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
        "  o2 = f3\n",
        "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
        "\n",
        "  # add the results of the upsampling and pool 3 prediction\n",
        "  o = tf.keras.layers.Add()([o2,o])\n",
        "\n",
        "  # upsample up to the size of the original image\n",
        "  o = tf.keras.layers.Conv2DTranspose(\n",
        "      n_classes,\n",
        "      kernel_size = (8,8),\n",
        "      strides = (8,8),\n",
        "      use_bias = False,\n",
        "      data_format = IMAGE_ORDERING\n",
        "  )(o)\n",
        "  o = tf.keras.layers.Cropping2D(((0, 0), (0, 96-84)))(o)\n",
        "\n",
        "  # append a sigmoid activation\n",
        "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
        "\n",
        "  return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJOhQz86Qk6n"
      },
      "source": [
        "### Define the Complete Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EJEf484312h",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# start the encoder using the default input size 64 x 84\n",
        "convs, img_input = FCN8()\n",
        "\n",
        "# pass the convolutions obtained in the encoder to the decoder\n",
        "dec_op = fcn8_decoder(convs, n_classes)\n",
        "\n",
        "# define the model specifying the input (batch of images) and output (decoder output)\n",
        "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GAenp1M4gXx"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAAXygZtbZmu"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpWpp8h4g_rE"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate = 0.01,momentum = 0.9,nesterov = True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510v0aVDXv1f"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HoZwpGWhMB-"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 170\n",
        "\n",
        "steps_per_epoch = 4000//BATCH_SIZE\n",
        "validation_steps = 800//BATCH_SIZE\n",
        "test_steps = 200//BATCH_SIZE\n",
        "\n",
        "\n",
        "history = model.fit(training_dataset,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    validation_data=validation_dataset, v\n",
        "                    alidation_steps=validation_steps,\n",
        "                    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih-Q7GoXzJe"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTkaFM2X1gr"
      },
      "source": [
        "### Make Predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zENjQuK0luH5"
      },
      "outputs": [],
      "source": [
        "results = model.predict(test_dataset, steps=test_steps)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwFiR9WAf0Av"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0,0])\n",
        "print(results[0,0,0,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Uj_uuV9TQt"
      },
      "outputs": [],
      "source": [
        "# Get the highest probability of each of these 11 slices and combine them in a single image by getting the argmax at this axis.\n",
        "results = np.argmax(results, axis=3)\n",
        "\n",
        "print(results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBeBwvHQd2pZ"
      },
      "outputs": [],
      "source": [
        "print(results[0,0,0])\n",
        "\n",
        "# prediction map for image 0\n",
        "print(results[0,:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpKDUuAWX5Pj"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "\n",
        "$$IOU = \\frac{area\\_of\\_overlap}{area\\_of\\_union}$$\n",
        "<br>\n",
        "$$Dice Score = 2 * \\frac{area\\_of\\_overlap}{combined\\_area}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKTpLmly_RXb"
      },
      "outputs": [],
      "source": [
        "def class_wise_metrics(y_true, y_pred):\n",
        "\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothing_factor = 0.00001\n",
        "\n",
        "  for i in range(n_classes):\n",
        "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (intersection) / (combined_area - intersection + smoothing_factor)\n",
        "    class_wise_iou.append(iou)\n",
        "\n",
        "    dice_score =  2 * ((intersection) / (combined_area + smoothing_factor))\n",
        "    class_wise_dice_score.append(dice_score)\n",
        "\n",
        "  return class_wise_iou, class_wise_dice_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWPwM4ZhHjE"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkbsk_P1fpRM",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# place a number here between 0 to 191 to pick an image from the test set\n",
        "integer_sliders = [1,50,80,82,105]\n",
        "\n",
        "for integer_slider in integer_sliders:\n",
        "  ds = test_dataset.unbatch()\n",
        "  ds = ds.batch(200)\n",
        "  images = []\n",
        "\n",
        "  y_true_segments = []\n",
        "  for image, annotation in ds.take(2):\n",
        "    y_true_segments = annotation\n",
        "    images = image\n",
        "\n",
        "\n",
        "  iou, dice_score = class_wise_metrics(np.argmax(y_true_segments[integer_slider], axis=2), results[integer_slider])\n",
        "  show_annotation_and_prediction(image[integer_slider], annotation[integer_slider], results[integer_slider], iou, dice_score)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiG9K4t6X9iZ"
      },
      "source": [
        "### Compute IOU Score and Dice Score of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2706boF0CNNS",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "cls_wise_iou, cls_wise_dice_score = class_wise_metrics(np.argmax(y_true_segments, axis=3), results)\n",
        "\n",
        "average_iou = 0.0\n",
        "for idx, (iou, dice_score) in enumerate(zip(cls_wise_iou[:-1], cls_wise_dice_score[:-1])):\n",
        "  print(\"Digit {}: IOU: {} Dice Score: {}\".format(idx, iou, dice_score))\n",
        "  average_iou += iou\n",
        "\n",
        "grade = average_iou * 10\n",
        "\n",
        "print(\"\\nAverage IOU is \" + str(grade))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "56d44d6a8424451b5ce45d1ae0b0b7865dc60710e7f74571dd51dd80d7829ee9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}