{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKMJ5ol8HlAosKpEM3lmPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeesem/Advanced-Tensorflow-Specialization/blob/main/OxfordPets_UNet_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "P6hY83zibX36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkZPAQGcbQmp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Oxford-IIIT Pets Dataset"
      ],
      "metadata": {
        "id": "06WXqtVZk5hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you hit a problem with checksums, you can execute the following line first\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=oxford_iiit_pet:3.1.0\n",
        "\n",
        "# download the dataset and get info\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
      ],
      "metadata": {
        "id": "6GM-pKvlk3k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible keys can access in the dataset dict\n",
        "# contains test and train splits\n",
        "print(dataset.keys())"
      ],
      "metadata": {
        "id": "EUPpQ_yIlGCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See information about the dataset\n",
        "print(info)"
      ],
      "metadata": {
        "id": "9waoCXifleQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the Dataset"
      ],
      "metadata": {
        "id": "kpDaRyV3ljnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Utilities\n",
        "\n",
        "# Data Augmentation\n",
        "def random_flip(input_image,input_mask):\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    input_mask = tf.image.flip_left_right(input_mask)\n",
        "\n",
        "  return input_image,input_mask\n",
        "\n",
        "# Data Normalization\n",
        "def normalize(input_image,input_mask):\n",
        "  input_image = tf.cast(input_image,tf.float32) / 255.0\n",
        "  input_mask -= 1\n",
        "  return input_image,input_mask\n",
        "\n",
        "@tf.function\n",
        "def load_image_train(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'],(128,128),method = 'nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'],(128,128),method = 'nearest')\n",
        "  input_image,input_mask = random_flip(input_image,input_mask)\n",
        "  input_image,input_mask = normalize(input_image,input_mask)\n",
        "\n",
        "  return input_image,input_mask\n",
        "\n",
        "def load_image_test(datapoint):\n",
        "  input_image = tf.image.resize(datapoint['image'],(128,128),method = 'nearest')\n",
        "  input_mask = tf.image.resize(datapoint['segmentation_mask'],(128,128),method = 'nearest')\n",
        "  input_image,input_mask = normalize(input_image,input_mask)\n",
        "\n",
        "  return input_image,input_mask"
      ],
      "metadata": {
        "id": "l9G0FjvFlmFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the train and test sets\n",
        "# num_parallel_calls: This parameter controls the level of parallelism. It determines how many\n",
        "# elements of the dataset are processed concurrently.\n",
        "# tf.data.experimental.AUTOTUNE is a special setting that allows TensorFlow to automatically decide the\n",
        "# optimal number of parallel calls. It uses available CPU resources to dynamically determine the best\n",
        "# parallelism level for performance, potentially speeding up data preprocessing.\n",
        "train = dataset['train'].map(load_image_train,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "test = dataset['test'].map(load_image_test)"
      ],
      "metadata": {
        "id": "_z3Tl2hyou6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Shuffle and group the train set into batches\n",
        "# train.cache(): caches the dataset in memory after the first iteration. This means that\n",
        "# subsequent epochs will not need to re-read the data from disk, which can speed up training.\n",
        "# The repeat() method repeats the dataset indefinitely. This is useful for training models where\n",
        "# you want the training to continue for multiple epochs without manually restarting the dataset iteration.\n",
        "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# Do a prefetch to optimzie preprocessing\n",
        "train_dataset = train_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# group the test set into batches\n",
        "test_dataset = test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Xtq36dzpp3EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Metrics Visualization"
      ],
      "metadata": {
        "id": "rk2wOR9jrhTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class list of the mask pixels\n",
        "class_names = ['pet','background','outline']\n",
        "\n",
        "def display_with_metrics(display_list,iou_list,dice_score_list):\n",
        "  metrics_by_id = [(idx,iou,dice_score) for idx,(iou,dice_score) in enumerate(zip(iou_list,dice_score_list)) if iou > 0.0]\n",
        "  metrics_by_id.sort(key = lambda tup : tup[1],reverse = True)\n",
        "\n",
        "  display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx],iou,dice_score) for idx,iou,dice_score in metrics_by_id]\n",
        "  display_string = \"\\n\\n\".join(display_string_list)\n",
        "\n",
        "  display(display_list,[\"Image\",\"Predicted Mask\",\"True Mask\"],display_string = display_string)\n",
        "\n",
        "def display(display_list,titles = [],display_string = None):\n",
        "  plt.figure(figsize = (15,15))\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1,len(display_list), i + 1)\n",
        "    plt.title(titles[i])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    if display_string and i == 1:\n",
        "      plt.xlabel(display_string,fontsize = 12)\n",
        "    img_arr = tf.keras.preprocessing.image.array_to_img(display_list[i])\n",
        "    plt.imshow(img_arr)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def show_image_from_dataset(dataset):\n",
        "\n",
        "  for image,mask in dataset.take(1):\n",
        "    sample_image,sample_mask = image,mask\n",
        "  display([sample_image,sample_mask],titles = [\"Image\",'True Mask'])\n",
        "\n",
        "def plot_metrics(metric_name,title,ylim = 5):\n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.plot(model_history.history[metric_name],color = 'blue',label = metric_name)\n",
        "  plt.plot(model_history.history['val_' + metric_name],color = 'green',label = 'val_' + metric_name)"
      ],
      "metadata": {
        "id": "QElYAF2Yrolm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display an image from the train set\n",
        "show_image_from_dataset(train)\n",
        "\n",
        "# Display an image from the test set\n",
        "show_image_from_dataset(test)"
      ],
      "metadata": {
        "id": "zcNQHRSbvJmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model\n",
        "\n",
        "With the dataset prepared, you can now build the UNet. Here is the overall architecture as shown in class:\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1BeQSKL2Eq6Fw9iRXsN1hgunY-CS2nH7V' alt='unet'>\n",
        "\n",
        "A UNet consists of an encoder (downsampler) and decoder (upsampler) with a bottleneck in between. The gray arrows correspond to the skip connections that concatenate encoder block outputs to each stage of the decoder. Let's see how to implement these starting with the encoder."
      ],
      "metadata": {
        "id": "MTLK6c_8vaKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "\n",
        "Like the FCN model you built in the previous lesson, the encoder here will have repeating blocks (red boxes in the figure below) so it's best to create functions for it to make the code modular. These encoder blocks will contain two Conv2D layers activated by ReLU, followed by a MaxPooling and Dropout layer. As discussed in class, each stage will have increasing number of filters and the dimensionality of the features will reduce because of the pooling layer.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1Gs9K3_8ZBn2_ntOtJL_-_ww4ZOgfyhrS' alt='unet'>"
      ],
      "metadata": {
        "id": "8huDbOVwvlSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Utilities\n",
        "def conv2d_block(input_tensor,n_filters,kernel_size = 3):\n",
        "  # First layer\n",
        "  x = input_tensor\n",
        "\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters = n_filters,\n",
        "        kernel_size = (kernel_size,kernel_size),\n",
        "        kernel_initializer = 'he_normal',\n",
        "        padding = 'same'\n",
        "    )(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "def encoder_block(inputs,n_filters = 64,pool_size = (2,2),dropout = 0.3):\n",
        "  f = conv2d_block(inputs,n_filters = n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size = pool_size)(f)\n",
        "  p = tf.keras.layers.Dropout(0.3)(p)\n",
        "\n",
        "  return f,p\n",
        "\n",
        "def encoder(inputs):\n",
        "  f1,p1 = encoder_block(inputs,n_filters = 64,pool_size = (2,2),dropout = 0.3)\n",
        "  f2,p2 = encoder_block(p1,n_filters = 128,pool_size = (2,2),dropout = 0.3)\n",
        "  f3,p3 = encoder_block(p2,n_filters = 256,pool_size = (2,2),dropout = 0.3)\n",
        "  f4,p4 = encoder_block(p3,n_filters = 512,pool_size = (2,2),dropout = 0.3)\n",
        "\n",
        "  return p4, (f1,f2,f3,f4)"
      ],
      "metadata": {
        "id": "HNHM38DvvfT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bottleneck"
      ],
      "metadata": {
        "id": "V11StqZpnI15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bottleneck(inputs):\n",
        "  bottle_neck = conv2d_block(inputs,n_filters = 1024)\n",
        "  return bottle_neck"
      ],
      "metadata": {
        "id": "_12sNNj7nHnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "Finally, we have the decoder which upsamples the features back to the original image size. At each upsampling level, you will take the output of the corresponding encoder block and concatenate it before feeding to the next decoder block. This is summarized in the figure below.\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1Ql5vdw6l88vxaHgk7VjcMc4vfyoWYx2w' alt='unet_decoder'>"
      ],
      "metadata": {
        "id": "xX6ihUnvnYMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Utilities\n",
        "\n",
        "def decoder_block(inputs,conv_output,n_filters = 64,kernel_size = 3,strides = 3,dropout = 0.3):\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters,kernel_size,strides = strides,padding = 'same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u,conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c,n_filters,kernel_size = 3)\n",
        "\n",
        "  return c\n",
        "\n",
        "def decoder(inputs,convs,output_channels):\n",
        "  f1,f2,f3,f4 = convs\n",
        "\n",
        "  c6 = decoder_block(inputs,f4,n_filters = 512,kernel_size = (3,3),strides = (2,2),dropout = 0.3)\n",
        "  c7 = decoder_block(c6,f3,n_filters = 256,kernel_size = (3,3),strides = (2,2),dropout = 0.3)\n",
        "  c8 = decoder_block(c7,f2,n_filters = 18,kernel_size = (3,3),strides = (2,2),dropout = 0.3)\n",
        "  c9 = decoder_block(c8,f1,n_filters = 64,kernel_size = (3,3),strides = (2,2),dropout = 0.3)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels,(1,1),activation = 'softmax')(c9)\n",
        "\n",
        "  return outputs"
      ],
      "metadata": {
        "id": "4JCwz---nW8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net"
      ],
      "metadata": {
        "id": "IsO3VExGtLXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "def unet():\n",
        "  # Specify the input shape\n",
        "  inputs = tf.keras.layers.Input(shape = (128,128,3,))\n",
        "\n",
        "  # Feed the input to the encoder\n",
        "  encoder_output,conv = encoder(inputs)\n",
        "\n",
        "  # feed th eencoder output to the bottlenect\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "\n",
        "  # feed the botttleneck and encoder block outputs to the encoder\n",
        "  outputs = decoder(bottle_neck,conv,output_channels = OUTPUT_CHANNELS)\n",
        "\n",
        "  # Create the model\n",
        "  model = tf.keras.Model(inputs = inputs,outputs = outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet()\n",
        "print(model.summary())\n",
        "plot_model(model,show_shapes = True,show_layer_names = True)"
      ],
      "metadata": {
        "id": "Fd5lez-QtQD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and Train the model"
      ],
      "metadata": {
        "id": "FhmXy-kRxjCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the optimizer, loss and metrics for training\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "G_dE9vAVxlSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the training parameters and train the model\n",
        "TRAIN_LENGTH = info.splits['train'].num_examples\n",
        "EPOCHS = 10\n",
        "VAL_SUBSPLITS = 5\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
        "\n",
        "model_history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = EPOCHS,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    validation_steps = VALIDATION_STEPS,\n",
        "    validation_data = test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "aoExMuq3yDtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation loss\n",
        "plot_metrics(\"loss\",title = \"Training vs Validation Loss\",ylim = 1)"
      ],
      "metadata": {
        "id": "PrYa9_CXydPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Predictions"
      ],
      "metadata": {
        "id": "F2e4hw2yxaf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction utilities\n",
        "def get_test_image_and_annotation_arrays():\n",
        "  ds = test_dataset.unbatch()\n",
        "  ds = ds.batch(info.splits['test'].num_examples)\n",
        "\n",
        "  images = []\n",
        "  y_true_segments = []\n",
        "\n",
        "  for image,annotation in ds.take(1):\n",
        "    y_true_segments = annotation.numpy()\n",
        "    images = image.numpy()\n",
        "\n",
        "  y_true_segments = y_true_segments[:(info.splits['test'].num_examples - - (info.splits['test'].num_examples % BATCH_SIZE))]\n",
        "  return images[:(info.splits['test'].num_examples - (info.splits['test'].num_examples % BATCH_SIZE))], y_true_segments\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask,axis = -1)\n",
        "  pred_mask = pred_mask[...,tf.newaxis]\n",
        "  return pred_mask[0].numpy()\n",
        "\n",
        "def make_prediciton(image,mask,num = 1):\n",
        "  image = np.reshape(image,(1,image.shape[0],image.shape[1],image.shape[2]))\n",
        "  pred_mask = model.predict(image)\n",
        "  pred_mask = create_mask(pred_mask)\n",
        "\n",
        "  return pred_mask"
      ],
      "metadata": {
        "id": "4hsNxUBIxeRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute class wise metrics"
      ],
      "metadata": {
        "id": "6b9vxyDs0VS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_wise_metrics(y_true,y_pred):\n",
        "  class_wise_iou = []\n",
        "  class_wise_dice_score = []\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "  for i in range(3):\n",
        "    intersection = np.sum((y_pred == i) * (y_trye == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
        "    dice_score = 2 * ( (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor) )\n",
        "    class_wise_iou.append(iou)\n",
        "    class_wise_dice_score.append(dice_score)"
      ],
      "metadata": {
        "id": "u1Zqxm6C0XsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the ground truth and predictions\n",
        "\n",
        "# get the ground truth from the test set\n",
        "y_true_images,y_true_segments = get_test_image_and_annotation_arrays()\n",
        "\n",
        "# feed the test set to the model to get the predicted mask\n",
        "results = model.predict(test_dataset,steps = info.splits['test'].num_examples // BATCH_SIZE)\n",
        "results = np.argmax(results,axis = 3)\n",
        "results = results[...,tf.newaxis]"
      ],
      "metadata": {
        "id": "34WJnloa1hnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the IOU for each class\n",
        "cls_wise_iou,cls_wise_dice_score = class_wise_metrics(y_true_segments,results)"
      ],
      "metadata": {
        "id": "hMj5EFWs2KmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the iou for each class\n",
        "for idx,iou in enumerate(cls_wise_iou):\n",
        "  space = \" \" * (10 - len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx],space,iou))"
      ],
      "metadata": {
        "id": "LQy3olEi2gNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dice score for each class\n",
        "for idx,dice_score in enumerate(cls_wise_dice_score):\n",
        "  space = \" \" * (10 - len(class_names[idx]) + 2)\n",
        "  print(\"{}{}{} \".format(class_names[idx],space,dice_score))"
      ],
      "metadata": {
        "id": "IV1mKQHZ3LRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show Predictions"
      ],
      "metadata": {
        "id": "3IF_0Nog3cPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integer_slider = 3646\n",
        "\n",
        "# Get the prediction_mask\n",
        "y_pred_mask = make_prediction(y_true_images[integer_slider],y_true_segments[integer_slider])\n",
        "\n",
        "# Compute the class wise metrics\n",
        "iou,dice_score = class_wise_metrics(y_true_segments[integer_slider],y_pred_mask)\n",
        "\n",
        "# Overlay the metrics with the images\n",
        "display_with_metrics(\n",
        "    [y_true_images[integer_slider],y_pred_mask,y_true_segments[integer_slider],iou,dice_score]\n",
        ")"
      ],
      "metadata": {
        "id": "ZSKLTae13fui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}