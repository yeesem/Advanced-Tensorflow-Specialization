{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qxq9uZAk3Lh"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MooRFGEeI1zb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import random\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9rq-0uk7nS"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjhN6GgfmUfx"
      },
      "outputs": [],
      "source": [
        "# set a random seed\n",
        "np.random.seed(51)\n",
        "\n",
        "# parameters for building the model and training\n",
        "BATCH_SIZE=2000\n",
        "LATENT_DIM=512\n",
        "IMAGE_SIZE=64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXTdjxmolDBo"
      },
      "source": [
        "## Download the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxKW6Q88KHcL"
      },
      "outputs": [],
      "source": [
        "# make the data directory\n",
        "try:\n",
        "  os.mkdir('/tmp/anime')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# download the zipped dataset to the data directory\n",
        "data_url = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\n",
        "data_file_name = \"animefaces.zip\"\n",
        "download_dir = '/tmp/anime/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "\n",
        "# extract the zip file\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6WCIlclWaA"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTlx97U_JDPB"
      },
      "outputs": [],
      "source": [
        "# Data Preparation Utilities\n",
        "\n",
        "def get_dataset_slice_paths(image_dir):\n",
        "  '''returns a list of paths to the image files'''\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "def map_image(image_filename):\n",
        "  '''preprocesses the images'''\n",
        "  img_raw = tf.io.read_file(image_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  # resize: Changes the spatial dimensions (width and height) of an image.\n",
        "  # Exp : have a 100x100 image and you resize it to 200x200, the image will be\n",
        "  # scaled up with interpolated values to fill the new dimensions.\n",
        "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "  image = image / 255.0\n",
        "  # Changes the shape of the tensor without altering the data.\n",
        "  # Exp : If you have a tensor of shape (100, 100, 3) and you reshape it to\n",
        "   # (50, 200, 3), the total number of elements (1001003) remains the same,\n",
        "   # but their arrangement is different.\n",
        "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoCJ6DPJHL8"
      },
      "outputs": [],
      "source": [
        "# get the list containing the image paths\n",
        "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
        "\n",
        "# shuffle the paths\n",
        "random.shuffle(paths)\n",
        "\n",
        "# split the paths list into to training (80%) and validation sets(20%).\n",
        "paths_len = len(paths)\n",
        "train_paths_len = int(paths_len * 0.8)\n",
        "\n",
        "train_paths = paths[:train_paths_len]\n",
        "val_paths = paths[train_paths_len:]\n",
        "\n",
        "# load the training image paths into tensors, create batches and shuffle\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
        "training_dataset = training_dataset.map(map_image)\n",
        "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "# load the validation image paths into tensors and create batches\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
        "validation_dataset = validation_dataset.map(map_image)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(f'number of batches in the training set: {len(training_dataset)}')\n",
        "print(f'number of batches in the validation set: {len(validation_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZRga9vlonx"
      },
      "source": [
        "## Display Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC1cpLViJLIu"
      },
      "outputs": [],
      "source": [
        "def display_faces(dataset, size=9):\n",
        "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
        "  dataset = dataset.unbatch().take(size)\n",
        "  n_cols = 3\n",
        "  n_rows = size//n_cols + 1\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  i = 0\n",
        "  for image in dataset:\n",
        "    i += 1\n",
        "    disp_img = np.reshape(image, (64,64,3))\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(disp_img)\n",
        "\n",
        "\n",
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Displays a row of images.'''\n",
        "  for idx, image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    image = np.reshape(image, shape)\n",
        "    plt.imshow(image)\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_predicted):\n",
        "  '''Displays input and predicted images.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eZsrZtqJOzv"
      },
      "outputs": [],
      "source": [
        "display_faces(validation_dataset, size=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSBtdCVim9aC"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQvzWaNqLrB1"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNxIUUS9ng9"
      },
      "source": [
        "### Sampling Class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-3qk6ZBm0Fl"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mu, sigma = inputs\n",
        "\n",
        "    # Get the size and dimensions of the batch\n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "\n",
        "    # Generate a random tensor\n",
        "    epsilon = tf.keras.backend.random_normal(shape = (batch,dim))\n",
        "\n",
        "    # Combine the inputs and noise\n",
        "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "\n",
        "    return  z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjCSa7Y-Gvk"
      },
      "source": [
        "### Encoder Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VSVYjDim4Dk"
      },
      "outputs": [],
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(32,3,strides = 2,padding = 'same',activation = 'relu')(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(64,3,strides = 2,padding = 'same',activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(128,3,strides = 2,padding = 'same',activation = 'relu')(x)\n",
        "  batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Flatten the features and feed into the Dense network\n",
        "  x = tf.keras.layers.Flatten()(batch_3)\n",
        "\n",
        "  x = tf.keras.layers.Dense(1024,activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Add output dense networks for mu and sigma, units equal to the declared latend_dim\n",
        "  mu = tf.keras.layers.Dense(latent_dim)(x)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim)(x)\n",
        "\n",
        "  return mu, sigma, batch_3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOy7wPPY-g-N"
      },
      "source": [
        "### Encoder Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Y-wLFym60N"
      },
      "outputs": [],
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape = input_shape)\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs,latent_dim = latent_dim)\n",
        "  z = Sampling()((mu,sigma))\n",
        "  model = tf.keras.Model(inputs = inputs,outputs = [mu,sigma,z])\n",
        "\n",
        "  model.summary()\n",
        "  return model, conv_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ENB-6a-0R5"
      },
      "source": [
        "### Decoder Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlTjAzgsm9Vn"
      },
      "outputs": [],
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "\n",
        "  # Feed to a Dense network with units computed from the conv_shpe dimensions\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = tf.keras.layers.Dense(units,activation = 'relu')(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Reshape output using the conv_shape _dimensions\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1],conv_shape[2],conv_shape[3]))(x)\n",
        "\n",
        "  # Upsample the features back to the original dimensions\n",
        "  x = tf.keras.layers.Conv2DTranspose(128,3,strides = 2,padding = 'same',activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2DTranspose(64,3,strides = 2,padding = 'same',activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Conv2DTranspose(32,3,strides = 2,padding = 'same',activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # Strides = 3, the channel of image == 3\n",
        "  x = tf.keras.layers.Conv2DTranspose(3,3,strides = 1,padding = 'same',activation = 'sigmoid')(x)\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLLz84r_MlN"
      },
      "source": [
        "### Decoder Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUgTyqNFm_jR"
      },
      "outputs": [],
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  # Set the inputs to the shape of the latent space\n",
        "  inputs = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "\n",
        "  # Get the output of the decoder layers\n",
        "  outputs = decoder_layers(inputs,conv_shape)\n",
        "\n",
        "  # Declare the inputs and outputs of the model\n",
        "  model = tf.keras.Model(inputs,outputs)\n",
        "\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps0yuE1d_cQc"
      },
      "source": [
        "### Kullback–Leibler Divergence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tngFmDDwnDn-"
      },
      "outputs": [],
      "source": [
        "def kl_reconstruction_loss(mu, sigma):\n",
        "\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  return tf.reduce_mean(kl_loss) * -0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuPHg28JnGCp"
      },
      "outputs": [],
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  # Sets the inputs\n",
        "  inputs = tf.keras.layers.Input(shape = input_shape)\n",
        "\n",
        "  # Get mu,sigma and z from the encoder output\n",
        "  mu,sigma,z = encoder(inputs)\n",
        "\n",
        "  # Get the reconstructed output from the decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # Define the inputs and outputs of the VAE\n",
        "  model = tf.keras.Model(inputs,reconstructed)\n",
        "\n",
        "  # Add the KL loss\n",
        "  loss = kl_reconstruction_loss(mu,sigma)\n",
        "  model.add_loss(loss)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnPo0Pr3nIts",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "  encoder,conv_shape = encoder_model(latent_dim,input_shape)\n",
        "  decoder = decoder_model(latent_dim,conv_shape)\n",
        "  vae = vae_model(encoder,decoder,input_shape = input_shape)\n",
        "\n",
        "  return encoder, decoder, vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHdr3CUznL5Z"
      },
      "outputs": [],
      "source": [
        "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IwN5vlAb5w"
      },
      "source": [
        "## Train the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHPwSmZFnQ_2"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGe445j0nTmf"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i, :, :, :] * 255\n",
        "      img = img.astype('int32')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvL1bHXJnajM"
      },
      "outputs": [],
      "source": [
        "# Training loop. Display generated images each epoch\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(training_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      # feed a batch to a VAE model\n",
        "      reconstructed = vae(x_batch_train)\n",
        "\n",
        "      # compute reconstruction loss\n",
        "      flattened_inputs = tf.reshape(x_batch_train,shape = [-1])\n",
        "      flattened_outputs = tf.reshape(reconstructed,shape = [-1])\n",
        "      loss = mse_loss(flattened_inputs,flattened_outputs) * 64 * 64 * 3\n",
        "\n",
        "      loss += sum(vae.losses)\n",
        "\n",
        "    grads = tape.gradient(loss,vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads,vae.trainable_weights))\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      display.clear_output(wait=False)\n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5wfzGfABny6"
      },
      "source": [
        "# Plot Reconstructed Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfIbqTIKSXEe"
      },
      "outputs": [],
      "source": [
        "test_dataset =  .take(1)\n",
        "output_samples = []\n",
        "\n",
        "for input_image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "idxs = np.random.choice(64, size=10)\n",
        "\n",
        "vae_predicted = vae.predict(test_dataset)\n",
        "display_results(output_samples[idxs], vae_predicted[idxs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YKUOCA5BtAA"
      },
      "source": [
        "# Plot Generated Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCpTybvGSS6L"
      },
      "outputs": [],
      "source": [
        "def plot_images(rows, cols, images, title):\n",
        "    '''Displays images in a grid.'''\n",
        "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
        "\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.imshow(grid)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# initialize random inputs\n",
        "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
        "\n",
        "# get predictions from the decoder model\n",
        "predictions= decoder.predict(test_vector_for_generation)\n",
        "\n",
        "# plot the predictions\n",
        "plot_images(8,8,predictions,'Generated Images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvw0HLY2kV3w"
      },
      "source": [
        "## Save the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX3LrJZveC8o"
      },
      "outputs": [],
      "source": [
        "vae.save(\"anime.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCd50-pubX_o"
      },
      "outputs": [],
      "source": [
        "# You can use this cell as a shortcut for downloading your model\n",
        "from google.colab import files\n",
        "files.download(\"anime.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}